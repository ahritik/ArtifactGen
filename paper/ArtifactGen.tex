\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025

% The authors should use one of these tracks.
% Before accepting by the NeurIPS conference, select one of the options below.
% 0. "default" for submission
% \usepackage{neurips_2025}
% the "default" option is equal to the "main" option, which is used for the Main Track with double-blind reviewing.
% 1. "main" option is used for the Main Track
%  \usepackage[main]{neurips_2025}
% 6. "dblblindworkshop" option is used for the Workshop with double-blind reviewing
\usepackage[dblblindworkshop]{neurips_2025}

% For workshops (5., 6.), the authors should add the name of the workshop, "\workshoptitle" command is used to set the workshop title.
\workshoptitle{Data on the Brain & Mind Findings}

% "preprint" option is used for arXiv or other preprint submissions
 % \usepackage[preprint]{neurips_2025}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{subfigure}

% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote. 
\title{ArtifactGen: Benchmarking WGAN-GP vs Diffusion for Label-Aware EEG Artifact Synthesis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{
  Hritik Arasu \\
  Department of Behavior and Brain Sciences\\
  University of Texas at Dallas\\
  Richardson, TX 75080 \\
  \texttt{hritik.arasu@UTDallas.edu} \\
  \And
  Faisal R. Jahangiri \\
  Department of Behavior and Brain Sciences\\
  University of Texas at Dallas\\
  Richardson, TX 75080 \\
  \texttt{faisal.jahangiri@utdallas.edu} \\
}

\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
Artifacts in electroencephalography (EEG)---muscle, eye movement, electrode, chewing, and shiver---confound automated analysis yet are costly to label at scale. We study whether modern generative models can synthesize realistic, label-aware artifact segments suitable for augmentation and stress-testing. Using the TUH EEG Artifact (TUAR) corpus, we curate subject-wise splits and fixed-length multi-channel windows (e.g., 250 samples) with preprocessing tailored to each model (per-window min--max for adversarial training; per-recording/channel $z$-score for diffusion). We compare a conditional WGAN-GP with a projection discriminator to a 1D denoising diffusion model with classifier-free guidance, and evaluate along three axes: (i) fidelity via Welch band-power deltas ($\Delta\delta,\ \Delta\theta,\ \Delta\alpha,\ \Delta\beta$), channel-covariance Frobenius distance, autocorrelation $L_2$, and distributional metrics (MMD/PRD); (ii) specificity via class-conditional recovery with lightweight $k$NN/classifiers; and (iii) utility via augmentation effects on artifact recognition. In our setting, WGAN-GP achieves closer spectral alignment and lower MMD to real data, while both models exhibit weak class-conditional recovery, limiting immediate augmentation gains and revealing opportunities for stronger conditioning and coverage. We release a reproducible pipeline---data manifests, training configurations, and evaluation scripts---to establish a baseline for EEG artifact synthesis and to surface actionable failure modes for future work.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
Artifacts in electroencephalography (EEG)—including muscle activity, eye movements, electrode noise, chewing, and shivering—routinely confound automated analysis and downstream clinical applications by distorting morphology, spectra, and cross-channel correlations. While artifact removal is well studied \citep{uriguen2015artifactremoval,jiang2019artifactremoval}, realistic \emph{synthesis} of artifact segments can complement curation efforts by enabling data augmentation, algorithm stress testing, and robustness benchmarking without additional human labeling. The challenge is to synthesize multi-channel windows that remain label-aware while respecting signal morphology, spectral structure, and channel covariance.

We introduce \textsc{ArtifactGen}, a practical and \emph{reproducible} framework for artifact-conditioned EEG synthesis built on subject-wise splits from the TUH EEG corpus and its artifact-annotated subset (TUAR) \citep{obeid2016tuh,hamid2020tuar}. \textsc{ArtifactGen} marries two complementary generative paradigms: (i) a conditional WGAN-GP with a projection discriminator for stable, label-aware synthesis \citep{gulrajani2017improved,miyato2018cgans}, and (ii) a denoising diffusion model using a 1D U-Net with FiLM-style conditioning \citep{perez2018film} and classifier-free guidance for controllability and sample quality \citep{ho2020denoising,ho2022classifierfree}. The pipeline standardizes preprocessing for fixed-length windows with configurable normalization, exposes training/evaluation via YAML configs, and ships analysis notebooks to facilitate faithful ablations and apples-to-apples comparisons.

Beyond single-number heuristics, \textsc{ArtifactGen} emphasizes a time-series-appropriate evaluation suite: (i) signal-level descriptors (e.g., Welch band-power deltas and covariance/ACF distances) to test morphology and spectra \citep{welch1967psd}; (ii) feature-space metrics (FID/KID/PRD) to quantify fidelity–coverage trade-offs \citep{heusel2017gans,binkowski2018demystifying,sajjadi2018assessing}; and (iii) functional tests—train-real/test-synth, train-synth/test-real, and AugMix-style augmentation—to probe utility and robustness \citep{hendrycks2020augmix}. We release code, configuration files, and notebooks to support rigorous baselining and community progress on EEG artifact generation and augmentation.

\subsection{Contributions}
\begin{itemize}
    \item A subject-wise pipeline to curate labeled artifact windows with robust normalization and fixed-length padding/truncation \citep{obeid2016tuh,hamid2020tuar}.
    \item A conditional WGAN-GP with projection discriminator for stable, label-aware synthesis \citep{gulrajani2017improved,miyato2018cgans}.
    \item A 1D diffusion model with FiLM conditioning and classifier-free guidance \citep{perez2018film,ho2020denoising,ho2022classifierfree}.
    \item A transparent evaluation suite spanning signal-level properties, feature-space distances (FID/KID/PRD), and functional tests including AugMix-style augmentation \citep{heusel2017gans,binkowski2018demystifying,sajjadi2018assessing,hendrycks2020augmix,welch1967psd}.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background}

Electroencephalography (EEG) is indispensable in clinical neurophysiology, yet real-world recordings are rife with non-neural artifacts—ocular movements, muscle activity, chewing, shivering, and electrode noise—that degrade downstream analysis and confound learning systems. Decades of signal-processing work have characterized these artifacts and proposed removal strategies, underscoring their broad spectral footprint and nonstationary morphology \citep{uriguengarciazapirain2015}. Large public corpora such as the Temple University Hospital EEG (TUH EEG) data \citep{obeid2016temple} and its artifact-focused subset, the Temple University Artifact Corpus (TUAR) \citep{hamid2020tuar}, enable supervised benchmarking but remain label- and condition-limited for training robust models that must generalize across subjects, montages, and acquisition conditions.

Generative modeling offers a complementary route: synthesize realistic artifact segments to (i) augment scarce classes, (ii) stress-test detector robustness, and (iii) study failure modes under controlled perturbations. Among competing paradigms, Generative Adversarial Networks (GANs) and Denoising Diffusion Probabilistic Models (DDPMs) dominate recent progress. GANs are sample-efficient but historically fragile; Wasserstein GANs with gradient penalty (WGAN-GP) improved stability and convergence by enforcing a soft Lipschitz constraint on the critic \citep{gulrajani2017improved}. For class-conditional generation, the projection discriminator embeds labels into the critic, providing a principled, label-aware training signal that scales well with many classes \citep{miyato2018cgans}. 

Diffusion models take an alternative path, learning to invert a gradual noising process and achieving state-of-the-art generative quality across domains \citep{ho2020denoising}. Practical refinements—including learned variance and hybrid training objectives—further reduce sampling cost while preserving fidelity \citep{nichol2021improved}. For conditional synthesis, classifier-free guidance yields strong label adherence with tunable trade-offs between diversity and faithfulness \citep{ho2022classifierfree}. Compared to GANs, diffusion models typically exhibit more stable training and better mode coverage, albeit with higher sampling latency—an important consideration for time-series pipelines.

Evaluating synthetic EEG requires metrics aligned with neurophysiological structure rather than image heuristics. Power spectral density (PSD) via Welch’s method provides band-power comparisons in canonical $\delta$/$\theta$/$\alpha$/$\beta$ bands, capturing key frequency-domain shifts induced by artifacts \citep{welch1967fft}. Temporal structure can be probed by autocorrelation statistics, while cross-channel dependencies—crucial in multi-lead EEG—are reflected in covariance distances. Complementary distributional tests quantify fidelity and coverage: precision–recall for distributions (PRD) disentangles sample quality from support coverage \citep{sajjadi2018assessing}, and maximum mean discrepancy (MMD) offers a kernel-based two-sample statistic sensitive to higher-order differences \citep{binkowski2018demystifying}. 

Within EEG specifically, recent surveys document growing use of GANs for augmentation and domain shifts across BCI and clinical tasks, while highlighting persistent gaps in label control, spectral realism, and reproducibility \citep{habashi2023ganeeg}. In parallel, compact discriminative backbones (e.g., EEGNet) provide downstream validators whose behavior on synthetic vs.\ real segments can reveal class-specific mismatches \citep{lawhern2018eegnet}. Together, these developments motivate a careful, \emph{label-aware} comparison between conditional WGAN-GP and conditional diffusion for artifact synthesis on TUAR, under subject-wise splits and an evaluation suite that balances spectral, temporal, multichannel, and distributional criteria.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}

\paragraph{Wasserstein GANs and conditioning for time series.}
The Wasserstein GAN with gradient penalty (WGAN-GP) stabilizes adversarial training by softly enforcing the 1-Lipschitz constraint \citep{gulrajani2017improved}. For semantic control, class-conditional GANs with a projection discriminator inject labels into the critic, improving fidelity and label-faithfulness without auxiliary classifiers \citep{miyato2018cgans}. In 1D signals (audio and other biosignals), fully convolutional generators and discriminators (e.g., WaveGAN) motivated architectural choices that preserve local stationarity while capturing long-range context \citep{donahue2018wavegan}. 

\paragraph{Diffusion models for (neuro)physiological time series.}
Denoising diffusion probabilistic models (DDPMs) learn to invert a fixed noising process and now set the bar for sample quality across domains \citep{ho2020denoising,nichol2021improved,dhariwal2021dmbeatgans}. Classifier-free guidance (CFG) trades off diversity and fidelity without a separate classifier, a practical tool for label-aware synthesis \citep{ho2022classifierfree}. Although most diffusion results target images, several works adapt them to time series via 1D U-Nets and score-based objectives; e.g., DiffWave for raw waveform synthesis and broader score-based SDE frameworks for sequences \citep{kong2020diffwave,song2020scoresde}. In neurophysiology specifically, recent models generate multichannel EEG/ECoG with strong realism and controllability \citep{vetter2024nddm,tosato2023eegdiffusion}. Our 1D U-Net with label conditioning follows this line, emphasizing artifact-aware control for EEG. We adopt FiLM-style conditioning to modulate intermediate features by condition vectors \citep{perez2018film}, and a U-Net backbone \citep{ronneberger2015unet} tailored to 1D signals.

\paragraph{EEG datasets and artifact corpora.}
We build on the Temple University Hospital EEG (TUH-EEG) ecosystem, the largest open clinical EEG collection \citep{obeid2016tuh}. For artifact-centric synthesis and evaluation, the TUH EEG Artifact Corpus (TUAR) provides dense annotations for common artifacts—eye movements, muscle, chewing, shiver, and electrode events—enabling subject-wise splits and label-aware benchmarking \citep{hamid2020tuar}. 

\paragraph{Evaluation of generative models for EEG.}
Image-native quality metrics such as FID \citep{heusel2017fid} and KID (polynomial-kernel MMD) \citep{binkowski2018kid}, and distributional precision/recall curves \citep{sajjadi2018prd,kynkaanniemi2019prdc} rely on features from a pretrained encoder; for EEG, we analogously extract features from artifact classifiers (e.g., EEGNet-style encoders) to adapt these ideas \citep{lawhern2018eegnet}. In addition, \emph{two-sample testing} provides principled sample–realism checks: kernel MMD \citep{gretton2012mmd} and classifier two-sample tests (C2ST), where a held-out accuracy near chance indicates good sample quality \citep{lopezpaz2016c2st}. Domain-aware signal metrics complement feature-space tests: Welch band-power deltas in canonical bands \citep{welch1967psd}, channel-covariance Frobenius distances, and ACF-based distances probe spectral shape, spatial coupling, and temporal dependence, respectively. We also report a simple 1-NN accuracy in the learned feature space as a pragmatic C2ST variant. 

\paragraph{Utility as the ultimate yardstick.}
Beyond proxy metrics, \emph{functional} evaluation—training downstream models with synthesized data—best captures whether synthetic artifacts help real tasks. Time-series work has advocated train-on-synthetic, test-on-real (TSTR) to quantify downstream utility \citep{yoon2019timegan}. In robustness-oriented vision, AugMix-style augmentation tests similarly relate synthetic perturbations to robustness improvements \citep{hendrycks2020augmix}. Our protocol prioritizes downstream artifact-recognition gains along with fidelity/specificity, in line with recent evidence that diffusion models tend to match or surpass GANs on both fidelity and coverage while remaining stable to train \citep{dhariwal2021dmbeatgans,nichol2021improved}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Dataset and Preprocessing}
\label{sec:data}
We curate EEG artifact segments from the Temple University Hospital EEG resources \citep{obeid2016temple}. To prevent subject leakage, we enforce \emph{subject-wise} splits with \textbf{149} training, \textbf{32} validation, and \textbf{32} test subjects. We consider five artifact classes throughout: \{\textbf{Muscle}, \textbf{Eye}, \textbf{Electrode}, \textbf{Chewing}, \textbf{Shiver}\}. All scripts are configuration-driven and reproducible.

\paragraph{Channels and sampling.}
We adopt a canonical eight-channel montage \(\{\text{Fp1}, \text{Fp2}, \text{C3}, \text{C4}, \text{O1}, \text{O2}, \text{T3}, \text{T4}\}\) at \(f_s = 250\) Hz. Only recordings with all required channels are admitted.

\paragraph{Windowing and overlap.}
Let \(x \in \mathbb{R}^{C \times T}\) denote a multi-channel clip (\(C{=}8\)). For a target window duration \(S\) seconds, the window length (in samples) is
\begin{equation}
L \;=\; \big\lfloor S \, f_s \big\rfloor .
\end{equation}
Windows are extracted with fractional overlap \(\rho \in [0,1)\) (default \(\rho{=}0.5\)), giving stride
\begin{equation}
s \;=\; \big\lfloor (1-\rho)\,L \big\rfloor .
\end{equation}
For an annotated interval of length \(T_i\) samples, the number of windows produced is
\begin{equation}
N_i \;=\; \max\!\Big(0,\; \Big\lfloor \frac{T_i - L}{s} \Big\rfloor + 1 \Big).
\end{equation}
Boundary fragments shorter than \(L\) are zero-padded; longer excerpts are truncated to exactly \(L\).
We use \(S{=}1.0\) s (\(L{=}250\)) for the adversarial path and \(S{=}2.0\) s (\(L{=}500\)) for the diffusion path.

\paragraph{Normalization (model-specific).}
Two normalization schemes are implemented and selected per run:
\begin{enumerate}
\item \textbf{Per-window min--max to \([-1,1]\) (adversarial path).} For window \(x \in \mathbb{R}^{C\times L}\) with global per-window extrema \(m=\min_{c,t} x_{c,t}\) and \(M=\max_{c,t} x_{c,t}\), we map
\begin{equation}
\hat{x}_{c,t} \;=\; 2\,\frac{x_{c,t}-m}{\max(M-m,\epsilon)} \;-\; 1, \qquad \epsilon = 10^{-8}.
\end{equation}
If configured, the pair \((m,M)\) is persisted with the window metadata to enable consistent inverse-rescaling at load time.
\item \textbf{Per-recording, per-channel \(z\)-score (diffusion path).} For channel \(c\) with mean \(\mu_c\) and standard deviation \(\sigma_c\) computed over the recording,
\begin{equation}
\tilde{x}_{c,t} \;=\; \frac{x_{c,t}-\mu_c}{\sigma_c + \epsilon}, \qquad \epsilon = 10^{-8}.
\end{equation}
\end{enumerate}

\paragraph{Filtering.}
Unless specified otherwise, we operate on \emph{raw} signals (no additional notch or band-pass filtering) to preserve artifact morphology; a filtered variant can be enabled without changing downstream loaders.

\paragraph{Manifests, class maps, and splits.}
We supply (i) a subject-wise split CSV ensuring disjoint identities across train/val/test; (ii) a stable class map for the five artifact labels; and (iii) a consolidated manifest (JSON) that records per-window paths, labels, subject IDs, normalization statistics, and the effective \(L\). These files fully reproduce dataset composition and preprocessing decisions.

\paragraph{Configuration (exact defaults).}
All data-related parameters are set via YAML and versioned with each run:
\begin{itemize}
\item \texttt{channels}: \([ \text{Fp1}, \text{Fp2}, \text{C3}, \text{C4}, \text{O1}, \text{O2}, \text{T3}, \text{T4} ]\), \quad \texttt{sample\_rate}: \(250\) Hz, \quad \texttt{overlap}: \(0.5\), \quad \texttt{filtering}: \texttt{raw}.
\item \textbf{Adversarial path (WGAN-GP):} \texttt{window\_seconds} \(= 1.0\), \texttt{length} \(= 250\), per-window min--max scaling to \([-1,1]\) with optional min/max persistence.
\item \textbf{Diffusion path (DDPM):} \texttt{window\_seconds} \(= 2.0\), \texttt{length} \(= 500\), per-recording, per-channel \(z\)-score normalization.
\item \texttt{split\_csv}: subject-wise split manifest; \texttt{class\_map\_csv}: five-class map; \texttt{manifest}: consolidated JSON written alongside results.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methods}

\subsection{Conditional WGAN-GP with Projection Discriminator}
We model artifact-conditioned synthesis as $G:\mathbb{R}^{d_z}\times\{1,\dots,K\}\!\to\!\mathbb{R}^{C\times T}$, where $z\!\sim\!\mathcal{N}(0,I)$ and $K$ is the number of artifact classes. For adversarial training we apply per-window min--max normalization to $[-1,1]$, concatenate $z$ with a one-hot label $y$, and upsample via a 1D transposed-convolutional generator to produce multi-channel windows $\tilde{x}$. 

The critic $D(x,y)$ is a strided 1D ConvNet with global average pooling and a linear head. Class awareness is injected via a projection term \citep{miyato2018cgans}:
\[
D(x,y)\;=\;w^\top \phi(x)\;+\;\langle \phi(x),\, e_y\rangle,
\]
with $\phi(x)\in\mathbb{R}^h$ the penultimate features and $e_y\in\mathbb{R}^h$ the learned class embedding. We optimize the Wasserstein objective with gradient penalty \citep{gulrajani2017improved}:
\[
\min_{G}\max_{D}\;\;\mathbb{E}_{x,y}[D(x,y)]-\mathbb{E}_{z,y}[D(G(z,y),y)]
\;+\;\lambda\,\mathbb{E}_{\hat x}\big(\lVert\nabla_{\hat x}D(\hat x,y)\rVert_2-1\big)^2,
\]
where $\hat x$ are linearly interpolated real/fake samples. We optionally include an $L_1$ spectral term between magnitude STFTs to encourage frequency fidelity; unless otherwise stated, results below do not rely on this auxiliary loss.

\subsection{Diffusion Model with 1D U\!-Net and FiLM Conditioning}
We adopt a denoising diffusion probabilistic model (DDPM) \citep{ho2020denoising} with a 1D U\!-Net backbone. Inputs $x\in\mathbb{R}^{C\times T}$ are standardized per recording/channel (z-score). Timestep embeddings (sinusoidal) and label embeddings are fused and injected via FiLM layers to modulate intermediate activations; we reserve a null label to support classifier-free guidance during sampling \citep{ho2022classifierfree}. The network predicts additive noise with an MSE loss.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.7\linewidth]{figs/examples_windows_multi.png}
  \caption{Representative multi-channel EEG windows for each artifact class, illustrating morphology and channel correlations after preprocessing.}
  \label{fig:examples-multi}
\end{figure}

\subsection{Training and Model Selection}
All models are implemented in PyTorch \citep{paszke2019pytorch}. For WGAN-GP we use Adam \citep{kingma2015adam} for both generator and critic with $n_{\text{critic}}>1$ and a configurable gradient-penalty coefficient. For DDPM we use AdamW \citep{loshchilov2019decoupled} and a linear $\beta$ schedule over $T$ steps. Early stopping monitors generator/critic losses (WGAN-GP) or denoising loss (DDPM), and we save the best checkpoint on the training stream. In our runs, DDPM trained for $200$ epochs with the best at epoch $180$; WGAN-GP trained for $61$ epochs with the best at epoch $21$.

\subsection{Evaluation}
We evaluate along three complementary axes using the statistics available in our current analysis.

\paragraph{Signal-level fidelity.}
We quantify spectral agreement via (i) \emph{bandwise relative error} between real and synthetic Welch bandpower in canonical bands $b\in\{\delta,\theta,\alpha,\beta,\gamma\}$,
\[
\mathrm{RelErr}_b \;=\; \frac{\big|\,P_b^{\text{fake}} - P_b^{\text{real}}\,\big|}{P_b^{\text{real}}+\varepsilon},
\]
reported separately for DDPM and WGAN, and (ii) a \emph{PSD $L_2$ error} that measures the squared $L_2$ distance between the average real and average synthetic power spectral density vectors (aggregated over windows). To capture basic amplitude biases we also report \emph{per-channel mean discrepancies}: for channel $c$, 
\[
\Delta\mu_c^{(\text{model})} \;=\; \mu^{\text{fake}}_c - \mu^{\text{real}}_c,
\]
tabulated as \texttt{d\_mu\_diff} (DDPM) and \texttt{g\_mu\_diff} (WGAN) alongside their corresponding aggregate magnitudes (\texttt{d\_mean\_effect}, \texttt{g\_mean\_effect}).

\begin{figure}[t]
  \centering
  \begin{minipage}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/embedding_tsne.png}\\
    \small (a) t\mbox{-}SNE embeddings (real vs synthetic).
  \end{minipage}\hfill
  \begin{minipage}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/embedding_umap.png}\\
    \small (b) UMAP embeddings (real vs synthetic).
  \end{minipage}
  \caption{Distributional alignment in embedding space. Comparison of (a) t\mbox{-}SNE and (b) UMAP projections of feature embeddings for real and synthetic segments; proximity and overlap indicate alignment across artifact classes.}
  \label{fig:embed-tsne-umap}
\end{figure}


\paragraph{Distributional similarity.}
We report the Maximum Mean Discrepancy (MMD) between sets of windows, including $\mathrm{MMD}(\text{R},\text{DDPM})$, $\mathrm{MMD}(\text{R},\text{WGAN})$, and $\mathrm{MMD}(\text{DDPM},\text{WGAN})$. For a characteristic kernel $k$, the unbiased empirical estimate over samples $\{x_i\}_{i=1}^m$ and $\{y_j\}_{j=1}^n$ is
\[
\widehat{\mathrm{MMD}}^2 \;=\; \tfrac{1}{m(m-1)}\!\!\sum_{i\neq i'}\! k(x_i,x_{i'}) \;+\; \tfrac{1}{n(n-1)}\!\!\sum_{j\neq j'}\! k(y_j,y_{j'}) \;-\; \tfrac{2}{mn}\!\sum_{i,j}\! k(x_i,y_j).
\]
Higher values indicate greater distributional divergence.

\paragraph{Diversity proxy.}
To assess sample variety we report a simple \emph{diversity} score defined as $1 - \overline{\mathrm{corr}}$, where $\overline{\mathrm{corr}}$ is the mean pairwise correlation across synthetic windows (computed over the same representation for all sets). Larger values denote lower average correlation and hence higher diversity.

\paragraph{Usage in this work.}
All metrics above are computed per model; bandwise relative errors are reported for each of $\delta/\theta/\alpha/\beta/\gamma$, channel-level mean discrepancies are provided for channels $c\!=\!0,\dots,4$, and global metrics include $\mathrm{MMD}$ (pairwise), PSD $L_2$ error, and the diversity proxy. These statistics are the basis of our quantitative comparisons between DDPM and WGAN in the present study.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}
Our head-to-head comparison of a conditional WGAN-GP with projection discriminator and a denoising diffusion model on TUH EEG artifacts surfaces three main themes: (i) \emph{fidelity at spectrum and channel level}, (ii) \emph{conditioning and normalization choices as first-order confounders}, and (iii) \emph{evaluation reliability beyond image-style heuristics}.

\paragraph{Spectral fidelity and distributional closeness.}
Across artifact classes, we observe consistently lower relative band-power errors for the WGAN compared to the diffusion model (e.g., $\delta\!\rightarrow\!\gamma$), and a smaller MMD to the real distribution (e.g., $\text{MMD}(\mathcal{R},\text{WGAN})<\text{MMD}(\mathcal{R},\text{DDPM})$). These results indicate that the adversarial prior, paired with a projection discriminator, more tightly matches second-order spectral structure than our diffusion baseline. Nevertheless, absolute gaps remain: our “Eval-Lite” summary shows non-trivial covariance Frobenius distances and ACF~$L_2$ discrepancies, signaling residual morphology and temporal-dependency mismatch even when band-power deltas are small. A trivial $1$-NN separability between real and synthetic also suggests that simple embeddings can still detect distribution shift; we therefore avoid over-interpreting degenerate PRD scores and emphasize metrics that remained stable across runs (band deltas, MMD, covariance/ACF).

\paragraph{Why might WGAN outperform here?}
Two design choices likely favored the WGAN: (i) per-window min–max scaling and shorter windows (1\,s) emphasize local amplitude dynamics and can act as an implicit spectral regularizer for the critic, and (ii) the projection discriminator injects labels in a way that directly shapes the decision boundary for artifact classes, improving \emph{conditional} alignment. By contrast, our diffusion configuration used z-score normalization per recording, longer windows (2\,s), and a relatively small 1D U-Net with $50$ sampling steps and $v$-prediction. In combination with classifier-free guidance (CFG), this can tilt the spectrum when guidance is set too aggressively and steps are limited, yielding the wider band-power errors we observed.

\paragraph{Channel effects and artifact specificity.}
While class-conditioned synthesis reflects the intended artifact at a coarse spectral level, per-channel mean shifts indicate systematic biases that vary by channel. This points to insufficient modeling of inter-channel covariance and montage-specific structure. In practice, artifacts such as eye movements and muscle bursts have characteristic topographies; better inductive bias for spatial coupling (e.g., grouped convolutions or graph layers over the montage) and explicit covariance regularization could reduce these channel-wise drifts.

\paragraph{Evaluation lessons.}
Standard image metrics (FID/PRD) are fragile for 1D neurophysiology. Our experience reinforced three best practices. First, compute domain-appropriate \emph{fidelity} measures (Welch band-power deltas, channel-covariance Frobenius, ACF~$L_2$). Second, quantify \emph{distributional closeness} via two-sample tools (MMD; C2ST) that can be audited. Third, isolate \emph{specificity/utility}: artifact-recovery via independent classifiers and downstream augmentation studies. We found PRD unstable under feature choices and class imbalance; by contrast, band deltas and covariance/ACF consistently ranked models and surfaced failure modes.

\paragraph{Limitations.}
Our comparison is not perfectly controlled: window length and normalization differ across models; diffusion sampling used only 50 steps; guidance scale and sampler were not exhaustively tuned; and the 1-D U-Net capacity was modest. Recovery experiments sometimes drew from a global real pool (rather than artifact-stratified pools), which can blunt specificity. Finally, we did not report confidence intervals for all metrics; future versions will include run-to-run variability and subject-wise bootstraps.

\paragraph{Implications and recommendations.}
For \emph{artifact synthesis at short horizons} (1–2\,s), a carefully tuned conditional WGAN-GP remains a strong baseline. For diffusion to close the gap, we recommend (i) more sampling steps or higher-order samplers; (ii) schedule/sampler co-design and EDM-style parameterization; (iii) careful CFG scaling and conditioner dropout; (iv) spectral-consistency objectives (e.g., auxiliary PSD loss) and artifact-aware augmentations during training; and (v) montage-aware architectures that directly model inter-channel structure. Beyond proxy fidelity, future work should prioritize \emph{downstream} endpoints (e.g., artifact-robust seizure detection), reported with uncertainty and subject-wise stratification.

\paragraph{Broader impact and safeguards.}
Synthetic EEG segments can reduce labeling burden and enable stress tests, but they also risk \emph{leakage} if trained on small subject pools. We mitigate this via subject-wise splits and recommend privacy checks (e.g., membership inference) before release. Any public models should document licenses, intended use, and limits, and avoid training on restricted clinical data without proper approvals.

\smallskip
\noindent\emph{Takeaway.} Under our settings, the projection-conditioned WGAN achieved tighter spectral alignment than the diffusion baseline, but both models leave detectable traces in temporal and cross-channel structure. Unifying preprocessing, upgrading diffusion schedules/samplers, and enforcing spectral/topographic consistency are the most promising levers for closing the gap.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Future Work}
Our immediate priority is to strengthen \emph{conditioning and guidance}. Beyond the current classifier-free guidance (CFG), we will benchmark classifier guidance and noise/sampler co-design to reduce mode collapse at high guidance scales and stabilize gradients in label-conditional settings \citep{dhariwal2021dmbeatgans,ho2022cfg,karras2022designspace}. We will also explore schedule-aware guidance and guidance mixing to better trade fidelity for diversity under tight sampling budgets.

\paragraph{Physiology-aware objectives.}
We plan to incorporate multi-resolution spectral objectives (e.g., STFT losses) to explicitly regularize band-power structure and reduce spectral artifacts, extending practices from neural audio generation to EEG \citep{yamamoto2020parallelwavegan}. For multi-channel realism, we will add constraints that preserve spatial covariance and cross-channel (phase) coupling, e.g., via coherency surrogates such as the imaginary part of coherency, which mitigates volume-conduction confounds \citep{nolte2004imagcoh}. These objectives complement time-domain losses used today.

\paragraph{Sampling efficiency.}
To make conditional diffusion practical for large EEG corpora and on-device synthesis, we will evaluate fast solvers and few/one-step generators, including DPM-Solver, progressive distillation, and consistency models \citep{lu2022dpmsolver,salimans2022distillation,song2023consistency}. We will pair these with EDM-style noise preconditioning and training-time design choices to maintain quality at low NFEs \citep{karras2022designspace}.

\paragraph{Evaluation beyond proxies.}
We will expand evaluation to \emph{representation spaces} by comparing embeddings from clinically relevant EEG encoders (e.g., EEGNet) to test whether conditional samples preserve task-relevant structure \citep{lawhern2018eegnet}. Distributional coverage will be quantified with precision/recall metrics for generative models and classifier two-sample tests, complementing PSD/covariance metrics \citep{kynkaanniemi2019pr,lopezpaz2016c2st}. Finally, we will emphasize \emph{utility} on downstream tasks (artifact detection; seizure false-alarm reduction) using TUAR/TUH-EEG settings and recent artifact–seizure pipelines \citep{ingolfsson2024artifactseizure,obeid2016tueg,hamid2020tuar,vetter2024neurodiffusion}.

\paragraph{Generalization and robustness.}
We will quantify cross-montage and cross-institution robustness by training on one TUAR version and testing on others (e.g., v2$\rightarrow$v3.0.1). We will also assess OOD robustness under distribution shifts in channel sets and hardware. For controllability, we plan multi-label conditioning (co-occurring artifacts) and continuous intensity controls to better match clinical variability.

\paragraph{Privacy and safety.}
Because synthetic clinical signals can leak training data, future releases will include privacy audits (membership inference, training-data extraction) and, where needed, mitigation (e.g., regularization or DP training) \citep{carlini2019secretsharer,carlini2023extracting,duan2023mia,matsumoto2023mia}. We will report privacy risk alongside fidelity/utility to set a stronger standard for clinical generative modeling.

\paragraph{Broader neurophysiology.}
Finally, we will adapt these conditioning, efficiency, and evaluation strategies to other neurophysiological modalities (ECoG, LFP, spiking) using recent diffusion architectures tailored to neural time series \citep{vetter2024neurodiffusion}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\section{References}
\nocite{*}
\bibliographystyle{plain}
\bibliography{ArtifactGen}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

\section{Appendices and Supplementary Material}

\subsection{Compute \& Environment}
\label{sec:compute}
All experiments were run on a single workstation; we provide exact hardware/software to support faithful reproduction.
\begin{itemize}
    \item \textbf{Hardware.} AMD Ryzen-class desktop (32 logical cores), 96~GB system RAM, 2~TB NVMe SSD, single NVIDIA RTX~4080 (16~GB). No multi-GPU or distributed training was used.
    \item \textbf{OS / Software Stack.} Pop!\_OS 22.04 LTS (Linux kernel 6.x), Python~3.12, PyTorch~2.2 with CUDA~12.1 toolchain, cuDNN~9, NumPy, SciPy, and scikit-learn (feature metrics / classifiers). Reproducibility scripts pin package versions in \texttt{requirements.txt}.
    \item \textbf{Diffusion (DDPM) model.} 1D U-Net with FiLM conditioning: channel widths (64, 128, 256), down/up depth 3, residual blocks with GroupNorm, sinusoidal timestep embedding fused with a learned class embedding (dim 13 including a null token for classifier-free guidance). EMA of model weights (decay 0.999) maintained for sampling.
    \item \textbf{GAN (WGAN-GP) model.} Transposed-convolution generator (latent $z \sim \mathcal{N}(0,I_{128})$ concatenated with one-hot class vector) with channel progression (128, 128, 64, 32, $C$); projection discriminator with mirrored strides and learned class embedding (dim 128). Optional STFT $L_1$ spectral auxiliary loss (disabled unless stated).
    \item \textbf{Optimization.} WGAN-GP: Adam ($\beta_1{=}0.5, \beta_2{=}0.9$), batch 256, critic steps $n_\text{critic}{=}5$, gradient penalty $\lambda_{gp}{=}10$. Diffusion: AdamW ($\beta_1{=}0.9, \beta_2{=}0.999$, weight decay $10^{-4}$), linear $\beta$ schedule with $T{=}1000$ training steps, sampling with 80-step deterministic DDIM-style schedule and classifier-free guidance scale 1.5.
    \item \textbf{Data pipeline.} Host-side prefetch and pinned memory enabled; each training window is $C{=}8$ channels with length 250 (WGAN-GP) or 500 (DDPM). GAN inputs are per-window min--max scaled to $[-1,1]$; diffusion inputs are per-recording $z$-scored per channel.
    \item \textbf{Sampling.} For quantitative evaluation we draw $N{=}3000$ windows per artifact class (5 classes) using EMA weights for diffusion and the best-FID checkpoint for WGAN-GP. Guidance (CFG) applied only in diffusion sampling; scale tuned on validation FID (best at 1.5).
    \item \textbf{Artifacts covered.} Five classes: \texttt{muscle}, \texttt{eye}, \texttt{electrode}, \texttt{chewing}, \texttt{shiver}. A ``none'' (clean) label is excluded from training to focus model capacity on artifact morphology.
    \item \textbf{Runtime.} Per-epoch wall-clock: WGAN-GP \(~2.1\) min, DDPM \(~3.4\) min. Full training (early stop) completes within 6--8 GPU hours per model; 15k synthetic samples (all classes) generate in \(<\!2\) min (WGAN-GP) vs. \(~6\) min (DDPM 80 steps).
    \item \textbf{Determinism.} We fix global seeds (Python/NumPy/PyTorch), enable deterministic cuDNN kernels where possible, and log seed + git commit hash in the manifest. Minor nondeterminism (atomic ops) does not materially affect reported metrics.
\end{itemize}

\subsection{Additional Figures}
% A1 — Additional qualitative examples
\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{figs/examples_windows.png}
  \caption{Additional qualitative example of the shiver class. Multi-channel windows highlighting morphology variety across artifacts beyond the main-text panel.}
  \label{fig:app-examples}
\end{figure}

% A2 — Per-file embeddings: t-SNE vs UMAP (side-by-side)
\begin{figure}[t]
  \centering
  \begin{minipage}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/tsne_perfile_summaries.png}\\
    \small (a) Per-file t\mbox{-}SNE summaries.
  \end{minipage}\hfill
  \begin{minipage}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/umap_perfile_summaries.png}\\
    \small (b) Per-file UMAP summaries.
  \end{minipage}
  \caption{Per-file embedding summaries. t\mbox{-}SNE (a) and UMAP (b) projections aggregated per recording, illustrating within-file cluster structure and variability.}
  \label{fig:app-embed-perfile}
\end{figure}

% A3 — Channel distribution across splits
\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{figs/channel_dist_per_split_multilabel.png}
  \caption{Channel distribution per split (multilabel). Relative presence of channels across train/val/test, useful for confirming split balance and avoiding channel leakage.}
  \label{fig:app-channel-dist}
\end{figure}

% A4 — Duration statistics (multilabel)
\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{figs/duration_boxplot_multilabel.png}
  \caption{Window duration statistics by artifact (multilabel). Boxplots summarize duration dispersion, complementing main-text descriptive stats.}
  \label{fig:app-duration-box}
\end{figure}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section*{NeurIPS Paper Checklist}

\begin{enumerate}

\item {\bf Claims}
    \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \item[] Answer: \answerYes{}
    \item[] Justification: The abstract and introduction precisely state the method and the evaluation scope and they avoid broader clinical claims.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the abstract and introduction do not include the claims made in the paper.
        \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
        \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
        \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
    \end{itemize}

\item {\bf Limitations}
    \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
    \item[] Answer: \answerYes{}
    \item[] Justification: A dedicated Limitations paragraph notes dataset constraints (TUAR only), potential covariate shift across sites/montages, limited ablations, and that downstream clinical utility is not established.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
        \item The authors are encouraged to create a separate "Limitations" section in their paper.
        \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
        \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
        \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
        \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
        \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
        \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
    \end{itemize}

\item {\bf Theory assumptions and proofs}
    \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
    \item[] Answer: \answerNA{}
    \item[] Justification: The paper is empirical and does not introduce new theorems or formal proofs; therefore no theoretical results or assumptions are claimed.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include theoretical results. 
        \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
        \item All assumptions should be clearly stated or referenced in the statement of any theorems.
        \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
        \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
        \item Theorems and Lemmas that the proof relies upon should be properly referenced. 
    \end{itemize}

    \item {\bf Experimental result reproducibility}
    \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
    \item[] Answer: \answerYes{}
    \item[] Justification: We specify datasets, preprocessing, splits, model architecture, conditioning, loss/schedules, samplers, hyperparameters, seeds where applicable, and provide end-to-end commands in the supplement to reproduce figures and tables.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
        \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
        \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
        \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
        \begin{enumerate}
            \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
            \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
            \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
            \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
        \end{enumerate}
    \end{itemize}


\item {\bf Open access to data and code}
    \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
    \item[] Answer: \answerYes{}
    \item[] Justification: Data is public (TUAR/TUEG) and paper contains pointers and preparation steps so others can recreate our pipelines.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that paper does not include experiments requiring code.
        \item Please see the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
        \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
        \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
        \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
        \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
    \end{itemize}


\item {\bf Experimental setting/details}
    \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
    \item[] Answer: \answerYes{}
    \item[] Justification: The Methods and Appendix enumerate data splits, montage, sampling rate, optimizer/schedule, diffusion steps/sampler, batch size, conditioning scheme, and selection rationales.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
        \item The full details can be provided either with the code, in appendix, or as supplemental material.
    \end{itemize}

\item {\bf Experiment statistical significance}
    \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
    \item[] Answer: \answerNo{}
    \item[] Justification: Current results report point estimates across runs/partitions without CIs; we note this and plan to include bootstrap CIs and seed variance in a revision.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
        \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
        \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
        \item The assumptions made should be given (e.g., Normally distributed errors).
        \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.
        \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.
        \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
        \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
    \end{itemize}

\item {\bf Experiments compute resources}
    \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
    \item[] Answer: \answerYes{}
    \item[] Justification: Appendix provides environment details
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
        \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
        \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
    \end{itemize}
    
\item {\bf Code of ethics}
    \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
    \item[] Answer: \answerYes{}
    \item[] Justification: We use de-identified, publicly available EEG datasets under their licenses.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
        \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
        \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
    \end{itemize}


\item {\bf Broader impacts}
    \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
    \item[] Answer: \answerYes{}
    \item[] Justification: We outline benefits and risks with mitigation notes.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that there is no societal impact of the work performed.
        \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
        \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
        \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
        \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
        \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
    \end{itemize}
    
\item {\bf Safeguards}
    \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
    \item[] Answer: \answerNA{}
    \item[] Justification: The released assets (training code and small domain-specific models) are not high-risk general-purpose generators; we nevertheless include usage notes and disclaimers but no gated access is required.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper poses no such risks.
        \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
        \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
        \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
    \end{itemize}

\item {\bf Licenses for existing assets}
    \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
    \item[] Answer: \answerYes{}
    \item[] Justification: We cite the data sources used as well as the packages.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not use existing assets.
        \item The authors should cite the original paper that produced the code package or dataset.
        \item The authors should state which version of the asset is used and, if possible, include a URL.
        \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.
        \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
        \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, \url{paperswithcode.com/datasets} has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
        \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
        \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.
    \end{itemize}

\item {\bf New assets}
    \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
    \item[] Answer: \answerNA{}.
    \item[] Justification: Did not provide any additional code apart from the instructions given.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not release new assets.
        \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
        \item The paper should discuss whether and how consent was obtained from people whose asset is used.
        \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
    \end{itemize}

\item {\bf Crowdsourcing and research with human subjects}
    \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? 
    \item[] Answer: \answerNA{}.
    \item[] Justification: No new human-subjects data or crowdsourcing were conducted; we rely solely on public de-identified datasets.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
        \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
    \end{itemize}

\item {\bf Institutional review board (IRB) approvals or equivalent for research with human subjects}
    \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
    \item[] Answer: \answerNA{}
    \item[] Justification: The work analyzes public, de-identified EEG datasets and does not involve direct human-subjects research requiring IRB review for this study.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
        \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
        \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
    \end{itemize}

\item {\bf Declaration of LLM usage}
    \item[] Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.
    %this research? 
    \item[] Answer: \answerNA{}
    \item[] Justification: LLMs are not part of the core technical method; any editorial assistance does not impact the scientific methodology and thus does not require declaration.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components.
        \item Please refer to our LLM policy (\url{https://neurips.cc/Conferences/2025/LLM}) for what should or should not be described.
    \end{itemize}

\end{enumerate}

\end{document}